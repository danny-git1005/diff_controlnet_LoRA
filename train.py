# -*- coding: utf-8 -*-
import os
import json
import copy
import argparse
import torch
import wandb
import traceback
from tqdm import tqdm
import matplotlib.pyplot as plt
import torchvision.transforms.functional as TF

# Diffusers å’Œç›¸é—œå‡½å¼åº«
from diffusers import ControlNetModel, StableDiffusionControlNetPipeline
from diffusers.optimization import get_scheduler
from accelerate import Accelerator, DistributedDataParallelKwargs # ç‚ºäº† find_unused_parameters
from torch.utils.data import DataLoader
from peft import LoraConfig # <-- Import LoraConfig
from peft import get_peft_model
from peft import PeftModel

# è‡ªè¨‚æ¨¡çµ„ (éœ€è¦ä½ æœ‰é€™äº›æª”æ¡ˆ)
from config import Config # å‡è¨­ä½ çš„è¨­å®šæª”åç‚º config.py
from utils.dataset import ControlNetDataset # å‡è¨­ä½ çš„è³‡æ–™é›†é¡åˆ¥åœ¨ utils/dataset.py

# ç’°å¢ƒè®Šæ•¸è¨­å®š
os.environ["NO_ALBUMENTATIONS_UPDATE"] = "1"
os.environ["TF_ENABLE_ONEDNN_OPTS"] = "0" # é¿å… TensorFlow OneDNN ç›¸é—œè­¦å‘Š


def parse_args():
    """è§£æå‘½ä»¤è¡Œåƒæ•¸"""
    parser = argparse.ArgumentParser(description="Train a ControlNet model using LoRA")
    parser.add_argument("--config", type=str, default="config.py", help="Path to config file")
    # --- å…è¨±å‘½ä»¤è¡Œè¦†è“‹ Config ä¸­çš„éƒ¨åˆ†è¨­å®š ---
    parser.add_argument("--condition_type", type=str, choices=["canny", "depth", "pose", "seg"], default=None,
                        help="Override config: Type of condition to use (aligns with config)")
    parser.add_argument("--use_text_condition", default=None, type=lambda x: (str(x).lower() == 'true'),
                        help="Override config: Use text prompts (True/False)")
    # --- LoRA åƒæ•¸è¦†è“‹ ---
    parser.add_argument("--use_lora", default=None, type=lambda x: (str(x).lower() == 'true'),
                        help="Override config: Use LoRA training (True/False)")
    parser.add_argument("--lora_rank", type=int, default=None, help="Override config: LoRA rank")
    parser.add_argument("--lora_alpha", type=int, default=None, help="Override config: LoRA alpha")
    parser.add_argument("--resume_from_checkpoint", type=str, default=None, help="Path to a saved training state checkpoint (directory). Use this to resume training.")
    parser.add_argument("--controlnet_checkpoint_path", type=str, default=None,help="Path to a specific ControlNet model checkpoint (directory). Use this to load only CN weights.")

    return parser.parse_args()

def main():
    args = parse_args()

    # --- è¼‰å…¥è¨­å®š ---
    # é€™éƒ¨åˆ†å‡è¨­ config.py è£¡æœ‰ä¸€å€‹ Config é¡åˆ¥
    # ä½ å¯èƒ½éœ€è¦æ ¹æ“šä½ çš„ config.py çµæ§‹èª¿æ•´é€™éƒ¨åˆ†
    try:
        # å‡è¨­ Config() ç›´æ¥å¯ä»¥ä½¿ç”¨
        config = Config()
        print("Loaded configuration from default config.py")
    except ImportError:
        print(f"Error: Could not import Config from config.py.")
        print("Please ensure config.py exists and defines a Config class.")
        return
    except Exception as e:
        print(f"Error loading config: {e}")
        return

    # --- ä½¿ç”¨å‘½ä»¤è¡Œåƒæ•¸æ›´æ–°è¨­å®š (å¦‚æœæä¾›äº†) ---
    if args.condition_type is not None:
        config.condition_type = args.condition_type
        print(f"Overridden config: condition_type = {config.condition_type}")
    if args.use_text_condition is not None:
        config.use_text_condition = args.use_text_condition
        print(f"Overridden config: use_text_condition = {config.use_text_condition}")
    if args.use_lora is not None:
        config.use_lora = args.use_lora
        print(f"Overridden config: use_lora = {config.use_lora}")
    if args.lora_rank is not None:
        config.lora_rank = args.lora_rank
        # é€šå¸¸ lora_alpha æœƒè·Ÿè‘— rank è®Šï¼Œé™¤éç‰¹åˆ¥æŒ‡å®š
        if args.lora_alpha is None:
            config.lora_alpha = args.lora_rank
        print(f"Overridden config: lora_rank = {config.lora_rank}")
    if args.lora_alpha is not None:
        config.lora_alpha = args.lora_alpha
        print(f"Overridden config: lora_alpha = {config.lora_alpha}")

    if args.resume_from_checkpoint is not None:
        config.resume_from_checkpoint = args.resume_from_checkpoint
        print(f"Overridden config: resume_from_checkpoint = {config.resume_from_checkpoint}")
    if args.controlnet_checkpoint_path is not None:
        config.controlnet_checkpoint_path = args.controlnet_checkpoint_path
        print(f"Overridden config: controlnet_checkpoint_path = {config.controlnet_checkpoint_path}")


    # --- æª¢æŸ¥æ˜¯å¦å•Ÿç”¨ LoRA ---
    if not hasattr(config, 'use_lora') or not config.use_lora:
        print("\nError: LoRA training is not enabled.")
        print("Please set 'use_lora = True' in your config file or pass '--use_lora True' via command line.")
        return

    print(f"\n--- Starting ControlNet LoRA Training ---")
    print(f"  Base Model: {config.base_model}")
    print(f"  ControlNet Model: {config.controlnet_model}")
    if args.resume_from_checkpoint:
        print(f" Resume from Checkpoint: {args.resume_from_checkpoint}")
    if args.controlnet_checkpoint_path:
        print(f" Load ControlNet Checkpoint: {args.controlnet_checkpoint_path}")

    print(f"  Condition Type: {config.condition_type if hasattr(config, 'condition_type') else 'Not Specified'}")
    print(f"  Use Text Condition: {config.use_text_condition}")
    print(f"  LoRA Rank: {config.lora_rank}")
    print(f"  LoRA Alpha: {config.lora_alpha}")
    print(f"  Mixed Precision: {config.mixed_precision}")
    print(f"  Output Directory: {config.output_dir}")
    print("-" * 40)

    # --- å»ºç«‹è¼¸å‡ºç›®éŒ„ ---
    os.makedirs(config.output_dir, exist_ok=True)

    # --- è¨­å®š Accelerator ---
    # If using DDP and getting errors about unused parameters, find_unused_parameters can help
    ddp_kwargs = DistributedDataParallelKwargs(find_unused_parameters=True)
    accelerator = Accelerator(
        mixed_precision=config.mixed_precision,
        gradient_accumulation_steps=config.gradient_accumulation_steps,
        log_with="wandb" if config.log_wandb else None, # æ•´åˆ wandb logging
        kwargs_handlers=[ddp_kwargs] # Handle potential DDP issues
    )
    if accelerator.is_main_process and config.log_wandb:
        try:
            if config.wandb_api_key:
                os.environ["WANDB_API_KEY"] = config.wandb_api_key
            
            wandb_entity = getattr(config, 'wandb_entity', 'default_entity')
            os.environ["WANDB_ENTITY"] = wandb_entity  # çµ¦ wandb tracker ä½¿ç”¨

            accelerator.init_trackers(
                project_name=config.project_name,
                config=vars(config)  # è¨˜éŒ„æ‰€æœ‰åƒæ•¸åˆ° W&B config
            )
            print("Weights & Biases initialized via Accelerator.")
        except Exception as e:
            print(f"Could not initialize WandB via Accelerator: {e}")
            config.log_wandb = False
    print(f"Accelerator device: {accelerator.device}")
    print(f"Number of processes: {accelerator.num_processes}")
    print(f"Mixed precision: {accelerator.mixed_precision}")

    # --- è¨­å®šè³‡æ–™é¡å‹ ---
    weight_dtype = torch.float32
    if accelerator.mixed_precision == "fp16":
        weight_dtype = torch.float16
    elif accelerator.mixed_precision == "bf16":
        weight_dtype = torch.bfloat16

    # --- è¼‰å…¥ ControlNet åŸºç¤æ¨¡å‹ ---
    try:
        controlnet = ControlNetModel.from_pretrained(
            config.controlnet_model,
            torch_dtype=weight_dtype # ä½¿ç”¨åŸºæ–¼ accelerator çš„ dtype
        )
        print(f"Loaded ControlNet base model from {config.controlnet_model}")
    except Exception as e:
        print(f"Error loading ControlNet model: {e}")
        return

    # --- è¼‰å…¥ Stable Diffusion Pipeline ---
    try:
        pipeline = StableDiffusionControlNetPipeline.from_pretrained(
            config.base_model,
            controlnet=controlnet, # å‚³å…¥åŸºç¤ ControlNet çµæ§‹
            torch_dtype=weight_dtype, # ä½¿ç”¨åŸºæ–¼ accelerator çš„ dtype
            low_cpu_mem_usage=False # é€šå¸¸åœ¨è¨“ç·´æ™‚è¨­ç‚º False é¿å…å•é¡Œ
        )
        # é—œé–‰ Safety Checker (å¸¸è¦‹æ–¼è¨“ç·´)
        pipeline.safety_checker = None
        # pipeline.safety_checker = lambda images, clip_input: (images, [False] * len(images)) # å¦ä¸€ç¨®é—œé–‰æ–¹å¼
        print(f"Loaded Stable Diffusion pipeline from {config.base_model}")
    except Exception as e:
        print(f"Error loading Stable Diffusion pipeline: {e}")
        return

    # --- ç§»å‹•éè¨“ç·´çµ„ä»¶åˆ°ç›®æ¨™è¨­å‚™ ---
    # VAE, Text Encoder, UNet åœ¨ LoRA è¨“ç·´ä¸­ä¸è¢«è¨“ç·´ï¼Œæå‰ç§»å‹•
    pipeline.vae.to(accelerator.device, dtype=weight_dtype)
    pipeline.text_encoder.to(accelerator.device, dtype=weight_dtype)
    pipeline.unet.to(accelerator.device, dtype=weight_dtype)
    print("Moved VAE, Text Encoder, UNet to accelerator device.")

    # --- å‡çµéè¨“ç·´æ¨¡å‹çš„æ¬Šé‡ ---
    pipeline.vae.requires_grad_(False)
    pipeline.text_encoder.requires_grad_(False)
    pipeline.unet.requires_grad_(False)
    # å‡çµ ControlNet çš„åŸå§‹æ¬Šé‡ (åœ¨åŠ å…¥ LoRA adapter ä¹‹å‰)
    controlnet.requires_grad_(False)
    print("Frozen weights for VAE, Text Encoder, UNet, and base ControlNet.")

    # --- ç‚º ControlNet æ·»åŠ  LoRA Adapters ---
    # è¨­å®š LoRA å±¤
    lora_controlnet_config = LoraConfig(
        r=config.lora_rank,
        lora_alpha=config.lora_alpha,
        init_lora_weights="gaussian", # æˆ–å…¶ä»–åˆå§‹åŒ–æ–¹æ³•ï¼Œä¾‹å¦‚ "pissa"
        target_modules=[
            # Transformer æ³¨æ„åŠ›å±¤çš„æ¨™æº–ç›®æ¨™
            "to_q",
            "to_k",
            "to_v",
            "to_out.0", # æ­£ç¢ºåœ°æŒ‡å®š ModuleList ä¸­çš„ç·šæ€§å±¤
            # Transformer å‰é¥‹ç¶²è·¯å±¤çš„æ¨™æº–ç›®æ¨™
            "ff.net.0.proj", # GEGLU å…§çš„ç·šæ€§å±¤
            "ff.net.2",      # æœ€çµ‚ç·šæ€§å±¤
            # å¯é¸ï¼šResNet å€å¡Šä¸­çš„æ™‚é–“åµŒå…¥æŠ•å½±å±¤
            # "time_emb_proj", # å–æ¶ˆè¨»è§£å³å¯åŒ…å«æ­¤å±¤
        ],
        lora_dropout=getattr(config, 'lora_dropout', 0.1), # å¾è¨­å®šè®€å– dropoutï¼Œè‹¥ç„¡å‰‡é è¨­ 0.1
        # bias="none" # æˆ– "all" æˆ– "lora_only"ã€‚è€ƒæ…®ä½¿ç”¨ 'none' æˆ– 'lora_only' ä»¥æé«˜æ•ˆç‡
    )

    try:
        # --- MODIFICATION: Wrap the controlnet using get_peft_model ---
        # This replaces the controlnet object with a PeftModel wrapper
        controlnet = get_peft_model(controlnet, lora_controlnet_config)
        print(f"Successfully wrapped ControlNet with PEFT LoRA adapters.")

        # --- MODIFICATION: Optional - Print trainable parameters using PEFT's method ---
        if accelerator.is_main_process:
            print("\nTrainable Parameters (PEFT):")
            controlnet.print_trainable_parameters() # PEFT helper function
        # --- MODIFICATION: Removed the controlnet.add_adapter() call as get_peft_model handles it ---

    except ValueError as ve: # Catch specific error related to target_modules
        print(f"Error wrapping ControlNet with PEFT: {ve}")
        print("This often means the 'target_modules' in LoraConfig are incorrect or not found.")
        print("Please carefully check the module names within your ControlNet structure.")
        # æç¤ºç”¨æˆ¶æª¢æŸ¥æ¨¡å‹çµæ§‹
        if accelerator.is_main_process:
            print("\nInspecting ControlNet Model Structure (use this to find target_modules):")
            # Print all module names and types recursively for detailed inspection
            for name, module in accelerator.unwrap_model(controlnet).named_modules(): # Use unwrap_model if needed
                print(f"- {name}: {type(module)}")
        return
    except Exception as e:
        print(f"Error wrapping ControlNet with PEFT: {e}")
        traceback.print_exc() # Print full traceback for other errors
        return


    # --- [å¯é¸] é©—è­‰å¯è¨“ç·´åƒæ•¸ ---
    if accelerator.is_main_process:
        print("\nTrainable Parameters (should be LoRA layers):")
        trainable_param_count = 0
        all_param_count = 0
        for name, param in controlnet.named_parameters():
            all_param_count += param.numel()
            if param.requires_grad:
                print(f"  [Trainable] {name} ({param.numel()})")
                trainable_param_count += param.numel()
        print("-" * 20)
        print(f"Total Parameters (ControlNet): {all_param_count:,}")
        print(f"Trainable Parameters (LoRA):   {trainable_param_count:,}")
        if all_param_count > 0:
             print(f"Trainable Ratio: {trainable_param_count / all_param_count:.4%}")
        print("-" * 20)

    # --- è¼‰å…¥æ–‡å­—æç¤º (å¦‚æœä½¿ç”¨) ---
    prompts = {}
    if config.use_text_condition:
        prompt_file = getattr(config, 'prompt_file', 'prompts.json') # å¾ config è®€å–ï¼Œé è¨­ prompts.json
        if os.path.exists(prompt_file):
            try:
                with open(prompt_file, 'r', encoding="utf-8") as f:
                    prompts = json.load(f)
                print(f"Loaded {len(prompts)} prompts from {prompt_file}")
            except Exception as e:
                print(f"Warning: Could not load prompts from {prompt_file}: {e}")
        else:
             print(f"Warning: Prompt file {prompt_file} not found. Text prompts might be missing.")

    # --- å»ºç«‹è³‡æ–™é›†å’Œè³‡æ–™è¼‰å…¥å™¨ ---
    try:
        train_dataset = ControlNetDataset(
            image_dir=config.image_dir,
            condition_dir=config.condition_dir,
            prompts=prompts, # å‚³å…¥è¼‰å…¥çš„æç¤º
            resolution=config.resolution,
            use_text_condition=config.use_text_condition
        )
        if len(train_dataset) == 0:
            print(f"Error: No data found in image_dir '{config.image_dir}' or condition_dir '{config.condition_dir}'. Please check paths.")
            return
        print(f"Created dataset with {len(train_dataset)} samples.")
    except Exception as e:
        print(f"Error creating dataset: {e}")
        return

    # --- [å¯é¸] åœ¨ä¸»é€²ç¨‹é¡¯ç¤ºç¯„ä¾‹è³‡æ–™ ---
    if accelerator.is_local_main_process: # åƒ…åœ¨æœ¬åœ°ä¸»é€²ç¨‹é¡¯ç¤º
        try:
            sample_idx = 0
            sample = train_dataset[sample_idx]
            image = sample["images"]
            condition_image = sample["condition_images"]
            prompt = sample["prompts"]

            # å°‡ Tensor è½‰å› PIL Image ä»¥ä¾¿é¡¯ç¤º
            if isinstance(image, torch.Tensor): image = TF.to_pil_image(image)
            if isinstance(condition_image, torch.Tensor): condition_image = TF.to_pil_image(condition_image)

            print(f"\nDisplaying Sample {sample_idx}:")
            print(f"  Prompt: '{prompt}'")
            fig, axes = plt.subplots(1, 2, figsize=(10, 5))
            axes[0].imshow(image); axes[0].set_title("Input Image"); axes[0].axis("off")
            axes[1].imshow(condition_image); axes[1].set_title("Condition Image"); axes[1].axis("off")
            plt.tight_layout();
            # å˜—è©¦ä¿å­˜åœ–åƒè€Œä¸æ˜¯é¡¯ç¤ºï¼Œä»¥é¿å… GUI å•é¡Œ
            sample_fig_path = os.path.join(config.output_dir, "sample_data.png")
            plt.savefig(sample_fig_path)
            print(f"  Saved sample data visualization to {sample_fig_path}")
            plt.close(fig) # é—œé–‰åœ–å½¢ï¼Œé‡‹æ”¾è³‡æº
        except Exception as e:
            print(f"Warning: Could not display/save sample image: {e}")


    # --- å»ºç«‹ DataLoader ---
    train_dataloader = DataLoader(
        train_dataset,
        batch_size=config.train_batch_size,
        shuffle=True,
        num_workers=getattr(config, 'num_workers', 4), # å¾è¨­å®šè®€å–ï¼Œé è¨­ 4
        pin_memory=True # é€šå¸¸èƒ½åŠ é€Ÿè³‡æ–™è½‰ç§»
    )

    # --- è¨­å®šå„ªåŒ–å™¨ ---
    # éæ¿¾åƒæ•¸ï¼Œåªå„ªåŒ–å¯è¨“ç·´çš„ LoRA åƒæ•¸
    try:
        trainable_params = list(filter(lambda p: p.requires_grad, controlnet.parameters()))
        if not trainable_params:
             print("Error: No trainable parameters found in ControlNet after adding LoRA adapter.")
             print("This might indicate an issue with LoraConfig target_modules or the adapter addition process.")
             return

        optimizer = torch.optim.AdamW(
            trainable_params,
            lr=config.learning_rate,
            betas=(getattr(config, 'adam_beta1', 0.9), getattr(config, 'adam_beta2', 0.999)),
            weight_decay=getattr(config, 'adam_weight_decay', 1e-2),
            eps=getattr(config, 'adam_epsilon', 1e-8),
        )
        print("Optimizer (AdamW) created for LoRA parameters.")
    except Exception as e:
        print(f"Error creating optimizer: {e}")
        return

    # --- è¨­å®šå­¸ç¿’ç‡æ’ç¨‹å™¨ ---
    lr_scheduler_type = getattr(config, 'lr_scheduler', "cosine") # å¾è¨­å®šè®€å–ï¼Œé è¨­ cosine
    num_warmup_steps = getattr(config, 'lr_warmup_steps', 0) # å¾è¨­å®šè®€å–ï¼Œé è¨­ 0

    lr_scheduler = get_scheduler(
        lr_scheduler_type,
        optimizer=optimizer,
        num_warmup_steps=num_warmup_steps * accelerator.num_processes, # è€ƒæ…®å¤š GPU warmup
        num_training_steps=config.max_train_steps * accelerator.num_processes, # ç¸½è¨“ç·´æ­¥æ•¸
    )
    print(f"Learning rate scheduler ({lr_scheduler_type}) created.")

    # --- ä½¿ç”¨ Accelerator æº–å‚™æ¨¡å‹ã€å„ªåŒ–å™¨ã€è³‡æ–™è¼‰å…¥å™¨å’Œæ’ç¨‹å™¨ ---
    # é‡è¦ï¼šåªæº–å‚™åŒ…å«å¯è¨“ç·´åƒæ•¸çš„æ¨¡å‹ (controlnet)
    controlnet, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(
        controlnet, optimizer, train_dataloader, lr_scheduler
    )
    print("Accelerator prepared ControlNet (LoRA), Optimizer, DataLoader, and LR Scheduler.")

    # --- è¨ˆç®—è¨“ç·´ç¸½è¼ªæ•¸ (Epochs) ---
    num_update_steps_per_epoch = len(train_dataloader) // config.gradient_accumulation_steps
    num_train_epochs = (config.max_train_steps + num_update_steps_per_epoch - 1) // num_update_steps_per_epoch
    print(f"\n--- Training Details ---")
    print(f"  Total Training Steps: {config.max_train_steps}")
    print(f"  Gradient Accumulation Steps: {config.gradient_accumulation_steps}")
    print(f"  Effective Batch Size per Device: {config.train_batch_size}")
    print(f"  Total Batch Size (across all devices): {config.train_batch_size * accelerator.num_processes * config.gradient_accumulation_steps}")
    print(f"  Number of Epochs: {num_train_epochs}")
    print(f"  Updates per Epoch: {num_update_steps_per_epoch}")
    print("-" * 40)

    # --- è¨“ç·´è¿´åœˆ ---
    global_step = 0
    first_epoch = 0 # TODO: Add support for resuming training from checkpoint

    # é€²åº¦æ¢åªåœ¨ä¸»é€²ç¨‹é¡¯ç¤º
    progress_bar = tqdm(
        range(global_step, config.max_train_steps),
        disable=not accelerator.is_local_main_process,
        desc="Training Steps"
    )

    best_loss = float("inf")

    print("\n--- Starting Training Loop ---")
    for epoch in range(first_epoch, num_train_epochs):
        controlnet.train() # è¨­å®š ControlNet (å« LoRA) ç‚ºè¨“ç·´æ¨¡å¼
        train_loss = 0.0

        for step, batch in enumerate(train_dataloader):
            # ä½¿ç”¨ Accelerator é€²è¡Œæ¢¯åº¦ç´¯ç©
            with accelerator.accumulate(controlnet):
                # --- æº–å‚™è¼¸å…¥è³‡æ–™ ---
                # å°‡åœ–åƒè½‰ç‚º float ä¸¦æ¨™æº–åŒ–åˆ° [0, 1]
                images = batch["images"].to(dtype=torch.float32) / 255.0
                condition_images = batch["condition_images"].to(dtype=torch.float32) / 255.0

                # å°‡è³‡æ–™ç§»å‹•åˆ°ç›®æ¨™è¨­å‚™ä¸¦è½‰æ›é¡å‹ (æ ¹æ“šæ··åˆç²¾åº¦è¨­å®š)
                images = images.to(device=accelerator.device, dtype=weight_dtype)
                condition_images = condition_images.to(device=accelerator.device, dtype=weight_dtype)
                # --- ç·¨ç¢¼åœ–åƒè‡³ Latent Space (ä½¿ç”¨ VAE) ---
                # VAE ä¸éœ€è¦æ¢¯åº¦
                with torch.no_grad():
                    # æ³¨æ„ï¼špipeline.vae å·²ç¶“åœ¨ç›®æ¨™è¨­å‚™å’Œ dtype ä¸Š
                    latents = pipeline.vae.encode(images).latent_dist.sample()
                    # ä½¿ç”¨ VAE çš„ scaling factor
                    latents = latents * pipeline.vae.config.scaling_factor

                # --- ç”¢ç”Ÿé›œè¨Šå’Œæ™‚é–“æ­¥ ---
                noise = torch.randn_like(latents)
                bsz = latents.shape[0]

                # ç‚º batch ä¸­çš„æ¯å€‹æ¨£æœ¬éš¨æ©Ÿé¸å–æ™‚é–“æ­¥
                timesteps = torch.randint(0, pipeline.scheduler.config.num_train_timesteps, (bsz,), device=latents.device)
                timesteps = timesteps.long()

                # å°‡é›œè¨Šæ·»åŠ åˆ° Latent ä¸­
                noisy_latents = pipeline.scheduler.add_noise(latents, noise, timesteps)

                # --- ç²å–æ–‡å­—åµŒå…¥ (å¦‚æœä½¿ç”¨) ---
                if config.use_text_condition:
                    prompts = batch["prompts"]
                else:
                    prompts = [""] * bsz # å¦‚æœä¸ç”¨æ–‡å­—ï¼Œå‰‡ä½¿ç”¨ç©ºæç¤º

                # Text Encoder ä¸éœ€è¦æ¢¯åº¦
                with torch.no_grad():
                    # æ³¨æ„ï¼špipeline.tokenizer å’Œ pipeline.text_encoder å·²åœ¨ç›®æ¨™è¨­å‚™å’Œ dtype ä¸Š
                    text_inputs = pipeline.tokenizer(
                        prompts,
                        padding="max_length",
                        max_length=pipeline.tokenizer.model_max_length,
                        truncation=True,
                        return_tensors="pt"
                    ).to(accelerator.device) # Tokenizer è¼¸å‡ºä¹Ÿéœ€ç§»åˆ°è¨­å‚™
                    # ç²å– text encoder çš„æœ€å¾Œéš±è—å±¤ç‹€æ…‹
                    encoder_hidden_states = pipeline.text_encoder(text_inputs.input_ids)[0]
                    # ç¢ºä¿é¡å‹åŒ¹é…
                    encoder_hidden_states = encoder_hidden_states.to(dtype=weight_dtype)


                # --- å‰å‘å‚³æ’­: ControlNet + UNet ---
                # ControlNet éœ€è¦æ¢¯åº¦ (å› ç‚º LoRA å±¤æ˜¯å¯è¨“ç·´çš„)
                # ä½¿ç”¨ accelerator åŒ…è£å¾Œçš„ controlnet
                down_block_res_samples, mid_block_res_sample = controlnet(
                    sample=noisy_latents,
                    timestep=timesteps,
                    encoder_hidden_states=encoder_hidden_states,
                    controlnet_cond=condition_images,
                    return_dict=False, # ç›´æ¥ç²å– tuple è¼¸å‡º
                )

                # æ³¨æ„ï¼špipeline.unet å·²åœ¨ç›®æ¨™è¨­å‚™å’Œ dtype ä¸Š
                # é æ¸¬é›œè¨Šæ®˜å·®
                model_pred = pipeline.unet(
                    noisy_latents,
                    timesteps,
                    encoder_hidden_states=encoder_hidden_states,
                    down_block_additional_residuals=[
                            sample.to(dtype=weight_dtype) for sample in down_block_res_samples # ç¢ºä¿ dtype åŒ¹é…
                    ] if isinstance(down_block_res_samples, (list, tuple)) else down_block_res_samples.to(dtype=weight_dtype),
                    mid_block_additional_residual=mid_block_res_sample.to(dtype=weight_dtype) # ç¢ºä¿ dtype åŒ¹é…
                ).sample

                # --- è¨ˆç®—æå¤± ---
                # ä½¿ç”¨ MSE æå¤±æ¯”è¼ƒé æ¸¬é›œè¨Šå’Œå¯¦éš›æ·»åŠ çš„é›œè¨Š
                # ç¢ºä¿æ¯”è¼ƒæ™‚ä½¿ç”¨ float32 ä»¥å¢åŠ ç©©å®šæ€§
                loss = torch.nn.functional.mse_loss(model_pred.float(), noise.float(), reduction="mean")

                # åŒ¯ç¸½æ‰€æœ‰é€²ç¨‹çš„æå¤±ä»¥ä¾›ç´€éŒ„ (ç”¨æ–¼é¡¯ç¤ºå¹³å‡æå¤±)
                avg_loss = accelerator.gather(loss.repeat(config.train_batch_size)).mean()
                train_loss += avg_loss.item() / config.gradient_accumulation_steps

                # --- åå‘å‚³æ’­ ---
                accelerator.backward(loss)

                # --- æ›´æ–°æ¬Šé‡ (åªåœ¨æ¢¯åº¦åŒæ­¥æ™‚åŸ·è¡Œ) ---
                if accelerator.sync_gradients:
                    # æ¢¯åº¦è£å‰ª (å¯é¸ä½†æ¨è–¦)
                    max_grad_norm = getattr(config, 'max_grad_norm', 1.0) # å¾è¨­å®šè®€å–ï¼Œé è¨­ 1.0
                    if max_grad_norm is not None:
                         # åªè£å‰ª controlnet çš„å¯è¨“ç·´åƒæ•¸ (LoRA)
                         accelerator.clip_grad_norm_(trainable_params, max_grad_norm)

                    optimizer.step()
                    lr_scheduler.step()
                    optimizer.zero_grad()

            # --- æ—¥èªŒè¨˜éŒ„èˆ‡é€²åº¦æ›´æ–° (åªåœ¨æ¢¯åº¦åŒæ­¥å¾ŒåŸ·è¡Œ) ---
            if accelerator.sync_gradients:
                progress_bar.update(1)
                global_step += 1

                # æº–å‚™æ—¥èªŒè³‡è¨Š
                logs = {"step_loss": avg_loss.item(), "lr": lr_scheduler.get_last_lr()[0]}
                progress_bar.set_postfix(**logs)

                # ä½¿ç”¨ accelerator è¨˜éŒ„åˆ° wandb (å¦‚æœå•Ÿç”¨)
                if config.log_wandb:
                    accelerator.log({"train_loss_step": avg_loss.item(), "lr": logs["lr"]}, step=global_step)

                # --- ä¿å­˜æª¢æŸ¥é» (LoRA æ¬Šé‡) ---
                save_steps = getattr(config, 'save_steps', 4) # å¾è¨­å®šè®€å–ï¼Œé è¨­ 4
                if train_loss < best_loss :
                    if accelerator.is_main_process: # åªæœ‰ä¸»é€²ç¨‹ä¿å­˜
                        save_path = os.path.join(config.output_dir, f"checkpoint-best")
                        os.makedirs(save_path, exist_ok=True)

                        # è§£åŒ…æ¨¡å‹ä»¥ç²å–åŸå§‹ PEFT æ¨¡å‹
                        unwrapped_controlnet = accelerator.unwrap_model(controlnet)
                        # ä¿å­˜ LoRA adapter æ¬Šé‡
                        lora_save_path = os.path.join(save_path, "controlnet_lora")
                        unwrapped_controlnet.save_pretrained(lora_save_path)

                        print(f"\nSaved LoRA checkpoint at step {global_step} to {lora_save_path}")

                        # å¯é¸ï¼šå¦‚æœéœ€è¦ï¼Œå¯ä»¥ä¿å­˜å„ªåŒ–å™¨å’Œæ’ç¨‹å™¨ç‹€æ…‹ä»¥ä¾›æ¢å¾©è¨“ç·´
                        # accelerator.save_state(save_path)

                # --- åŸ·è¡Œé©—è­‰ (å¦‚æœè¨­å®šäº†æ­¥æ•¸) ---
                # validation_steps = getattr(config, 'validation_steps', None)
                # if validation_steps and global_step % validation_steps == 0:
                    if accelerator.is_main_process: # åªæœ‰ä¸»é€²ç¨‹åŸ·è¡Œé©—è­‰å’Œè¨˜éŒ„
                        print(f"\nRunning validation at step {global_step}...")
                        try:
                            # --- æº–å‚™é©—è­‰ ---
                            # ç²å–æœ€æ–°çš„ LoRA æ¬Šé‡
                            controlnet_for_infer = copy.deepcopy(controlnet)

                            # Merge ç”¨æ–¼é©—è­‰çš„é‚£ä¸€ä»½
                            if isinstance(controlnet_for_infer, PeftModel):
                                print("Merging LoRA into base model (copy) for validation...")
                                controlnet_for_infer = controlnet_for_infer.merge_and_unload()

                            # ç”¨é€™ä»½åšé©—è­‰
                            pipeline.controlnet = controlnet_for_infer
                            
                            # ç¢ºä¿ pipeline åœ¨æ­£ç¢ºçš„è¨­å‚™ä¸Š
                            pipeline.to(accelerator.device)
                            # è¨­å®šç‚ºè©•ä¼°æ¨¡å¼
                            pipeline.controlnet.eval()
                            # pipeline.unet.eval() # UNet å§‹çµ‚æ˜¯ eval ç‹€æ…‹

                            # --- å¾è³‡æ–™é›†ä¸­é¸å–æ¨£æœ¬é€²è¡Œé©—è­‰ --- # <-- MODIFICATION: Changed logic
                            num_val_samples = getattr(config, 'num_validation_images', 4) # å¾è¨­å®šè®€å–ï¼Œé è¨­ 4
                            # Clamp num_val_samples to dataset size if necessary
                            num_val_samples = min(num_val_samples, len(train_dataset))

                            val_condition_images = []
                            val_prompts = []
                            val_original_indices = [] # <-- MODIFICATION: Store original indices for logging/saving

                            # Ensure we have a dataset to sample from
                            if len(train_dataset) > 0:
                                # Use random indices for variety each time
                                val_indices = torch.randperm(len(train_dataset))[:num_val_samples].tolist()

                                print(f"Sampling validation data from dataset indices: {val_indices}")

                                for idx in val_indices:
                                    try:
                                        # Get the full sample dictionary from the dataset
                                        sample = train_dataset[idx]

                                        # --- Get Condition Image ---
                                        condition_img_tensor = sample["condition_images"]
                                        # Preprocess: ensure tensor, correct dtype, scale, unsqueeze batch dim
                                        if isinstance(condition_img_tensor, torch.Tensor):
                                            condition_img = condition_img_tensor.to(dtype=weight_dtype).div(255.0).unsqueeze(0)
                                            val_condition_images.append(condition_img.to(accelerator.device))
                                        else:
                                            print(f"Warning: Condition image at index {idx} is not a tensor. Skipping this sample.")
                                            continue # Skip if condition image is not valid

                                        # --- Get Prompt ---
                                        prompt = sample["prompts"] 
                                        # Basic check: Ensure it's a string
                                        if not isinstance(prompt, str):
                                            print(f"Warning: Prompt at index {idx} is not a string ('{prompt}'). Using empty string.")
                                            prompt = "" # Use empty string as fallback
                                        val_prompts.append(prompt)

                                        # Store the original index
                                        val_original_indices.append(idx)

                                    except Exception as e:
                                        print(f"Warning: Error retrieving validation sample at index {idx}: {e}. Skipping.")
                            else:
                                print("Warning: train_dataset is empty. Cannot perform validation.")
                        
                            # --- ç”Ÿæˆé©—è­‰åœ–åƒ ---
                            if val_condition_images and val_prompts: # Check if we have valid pairs
                                validation_images_log = []

                                # <-- MODIFICATION: Iterate through the collected pairs -->
                                for i, (prompt, condition_image_val) in enumerate(zip(val_prompts, val_condition_images)):
                                    original_idx = val_original_indices[i] # Get the original index
                                    print(f"  Generating validation image {i+1}/{len(val_prompts)} using prompt: '{prompt}' (from index {original_idx})")
                                    print(f"  Condition image shape: {condition_image_val.shape}")  
                                    print(f"  Condition image dtype: {condition_image_val.dtype}")
                               
                                    with torch.no_grad():
                                        try: # Add try-except around pipeline call
                                            val_image = pipeline(
                                                prompt=prompt, # Use the prompt from the dataset sample
                                                image=condition_image_val, # Use the condition image from the dataset sample
                                                num_inference_steps=config.num_inference_steps,
                                                guidance_scale=config.guidance_scale,
                                                controlnet_conditioning_scale=config.controlnet_conditioning_scale,
                                            ).images[0]
                                            print(f"  Validation image generated successfully.")
                                            # è¨˜éŒ„åˆ° WandB (å¦‚æœå•Ÿç”¨)
                                            if config.log_wandb:
                                                validation_images_log.append(wandb.Image(
                                                    val_image,
                                                    caption=f"Step {global_step} [Dataset Idx {original_idx}]: {prompt}"
                                                ))

                                            # å¯é¸ï¼šä¿å­˜é©—è­‰åœ–åƒåˆ°æœ¬åœ°æ–‡ä»¶
                                            # <-- MODIFICATION: Use original index in filename -->
                                            val_img_save_path = os.path.join(config.output_dir, f"validation_step_{global_step}_idx_{original_idx}.png")
                                            print(f"  Saving validation image to {val_img_save_path}")
                                            val_image.save(val_img_save_path)
                                        except Exception as pipe_e:
                                            print(f"  Error generating validation image for index {original_idx}: {type(pipe_e).__name__} - {pipe_e}")
                                            print("  ğŸ” ControlNet type:", type(pipeline.controlnet))
                                            traceback.print_exc()  # ğŸ” å°å‡ºå®Œæ•´éŒ¯èª¤å †ç–Š

                                # --- è¨˜éŒ„åˆ° WandB ---
                                if config.log_wandb and validation_images_log:
                                    accelerator.log({"validation_samples": validation_images_log}, step=global_step)
                                    print(f"Logged {len(validation_images_log)} validation images to WandB.")
                                elif not config.log_wandb and val_prompts: # Check if validation ran
                                    print(f"Generated {len(val_prompts)} validation images locally.")
                            else:
                                print("Skipping validation image generation as no valid data pairs were found.")

                        except Exception as e:
                            print(f"Error during validation setup or sampling: {e}")
                            traceback.print_exc() # Print detailed traceback for debugging
                        finally:
                            # --- æ¢å¾©è¨“ç·´ç‹€æ…‹ ---
                            # Make sure controlnet is back on the correct device if moved during validation
                            pipeline.controlnet.to(accelerator.device)
                            pipeline.controlnet.train() # å°‡ ControlNet è¨­ç½®å›è¨“ç·´æ¨¡å¼
                            print("Validation complete. Resuming training...")


            # --- æª¢æŸ¥æ˜¯å¦é”åˆ°æœ€å¤§æ­¥æ•¸ ---
            if global_step >= config.max_train_steps:
                break # è·³å‡ºå…§éƒ¨è¿´åœˆ (step loop)

        # åœ¨æ¯å€‹ epoch çµæŸæ™‚è¨˜éŒ„å¹³å‡è¨“ç·´æå¤±
        epoch_avg_loss = train_loss / num_update_steps_per_epoch if num_update_steps_per_epoch > 0 else 0.0
        if config.log_wandb:
            accelerator.log({"train_loss_epoch": epoch_avg_loss, "epoch": epoch}, step=global_step)
        print(f"Epoch {epoch+1}/{num_train_epochs} finished. Average Loss: {epoch_avg_loss:.6f}")


        # --- æª¢æŸ¥æ˜¯å¦é”åˆ°æœ€å¤§æ­¥æ•¸ ---
        if global_step >= config.max_train_steps:
            print(f"\nReached max_train_steps ({config.max_train_steps}). Stopping training.")
            break # è·³å‡ºå¤–éƒ¨è¿´åœˆ (epoch loop)

    # --- è¨“ç·´çµæŸ ---
    accelerator.wait_for_everyone() # ç­‰å¾…æ‰€æœ‰é€²ç¨‹å®Œæˆ

    # --- ä¿å­˜æœ€çµ‚çš„ LoRA æ¨¡å‹ ---
    if accelerator.is_main_process:
        final_save_path = os.path.join(config.output_dir, "final-lora-model")
        os.makedirs(final_save_path, exist_ok=True)

        # è§£åŒ…æ¨¡å‹ä¸¦ä¿å­˜ LoRA æ¬Šé‡
        unwrapped_controlnet = accelerator.unwrap_model(controlnet)
        lora_final_save_path = os.path.join(final_save_path, "controlnet_lora")
        unwrapped_controlnet.save_pretrained(lora_final_save_path)
        print(f"\nFinal LoRA weights saved to {lora_final_save_path}")

        # å¯é¸ï¼šä¿å­˜å®Œæ•´çš„ pipeline ç‹€æ…‹ï¼ˆåŒ…å« LoRA æ¬Šé‡ï¼‰
        # pipeline.controlnet = unwrapped_controlnet # ç¢ºä¿ pipeline ä½¿ç”¨æœ€çµ‚æ¬Šé‡
        # pipeline.save_pretrained(final_save_path)
        # print(f"Final pipeline state saved to {final_save_path}")


    # --- çµæŸ Accelerator å’Œ WandB ---
    accelerator.end_training()
    if config.log_wandb:
        wandb.finish()

    print("\n--- LoRA Training Completed Successfully! ---")

if __name__ == "__main__":
    main()